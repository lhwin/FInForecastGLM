一、大模型常识面
1.1 简单 介绍一下 大模型【LLMs】？
大模型：一般指1亿以上参数的模型，但是这个标准一直在升级，目前万亿参数以上的模型也有了。大语言模型（LargeLanguage
Model，LLM）是针对语言的大模型。
*
1.2 大模型【LLMs】后面跟的 175B、60B、540B等 指什么？
175B、60B、540B等：这些一般指参数的个数，B是Billion/十亿的意思，175B是1750亿参数，这是ChatGPT大约的参数规模。
*
1.3 大模型【LLMs】具有什么优点？
. 可以利用大量的无标注数据来训练一个通用的模型，然后再用少量的有标注数据来微调模型，以适应特定的任务。这种预训
练和微调的方法可以减少数据标注的成本和时间，提高模型的泛化能力；
. 可以利用生成式人工智能技术来产生新颖和有价值的内容，例如图像、文本、音乐等。这种生成能力可以帮助用户在创意、
娱乐、教育等领域获得更好的体验和效果；
. 可以利用涌现能力（EmergentCapabilities）来完成一些之前无法完成或者很难完成的任务，例如数学应用题、常识推理、
符号操作等。这种涌现能力可以反映模型的智能水平和推理能力。
*
1.4 大模型【LLMs】具有什么缺点？
. 需要消耗大量的计算资源和存储资源来训练和运行，这会增加经济和环境的负担。据估计，训练一个GPT-3模型需要消耗约
30万美元，并产生约284吨二氧化碳排放；
. 需要面对数据质量和安全性的问题，例如数据偏见、数据泄露、数据滥用等。这些问题可能会导致模型产生不准确或不道德
的输出，并影响用户或社会的利益；
. 需要考虑可解释性、可靠性、可持续性等方面的挑战，例如如何理解和控制模型的行为、如何保证模型的正确性和稳定性、
如何平衡模型的效益和风险等。这些挑战需要多方面的研究和合作，以确保大模型能够健康地发展。
*
二、大模型强化学习面
2.1 简单介绍强化学习？
强化学习：（ReinforcementLearning）一种机器学习的方法，通过从外部获得激励来校正学习方向从而获得一种自适应的学习能力。
*
2.2 简单介绍一下 RLHF？
基于人工反馈的强化学习（ReinforcementLearningfromHumanFeedback，RLHF）：构建人类反馈数据集，训练一个激励模型，
模仿人类偏好对结果打分，这是GPT-3后时代大语言模型越来越像人类对话核心技术。
*
三、大模型【LLMs】微调篇
3.1 大模型【LLMs】泛化问题？
泛化（Generalization）模型泛化是指一些模型可以应用（泛化）到其他场景，通常为采用迁移学习、微调等手段实现泛化。
*
3.2 大模型【LLMs】微调问题？
微调（FineTuning）针对大量数据训练出来的预训练模型，后期采用业务相关数据进一步训练原先模型的相关部分，得到准确度更
高的模型，或者更好的泛化。
通过在一个已经训练好的模型的基微调，可以让模型更专注于新任务或领域的特点，提高模型的性能和准确度。微调也可以让模型
更容易适应不同的输入输出格式，以及不同的评估指标。
*
3.3 大模型【LLMs】微调有哪些优点？
微调是一种常用的迁移学习方法，它可以利用预训练模型的通用知识，同时减少训练时间和数据需求。
在大语言模型上，微调是指在预训练好的大型语言模型基础上，针对特定任务进行额外训练。
这种方法需要对模型进行额外的训练，但可以提高模型在特定任务上的性能。微调通常用于解决那些无法通过提示工程解决的问题。
换句话说：它通过输入额外的样本，对模型部分参数进行修改，从而强化模型某部分能力。本质上也是一种引导和激发模型能力的
方法。
*
3.4 什么是大模型指令微调问题？
指令微调（InstructionFineTuning）是指 对已经存在的预训练模型，给出额外的指令或者标注数据集来提升模型的性能。
*
四、大模型【LLMs】\n思维链篇 4.1 什么是大模型的思维链？
思维链（Chain-of-Thought，CoT）：通过让大语言模型（LLM）将一个问题拆解为多个步骤，一步一步分析，逐步得出正确答案。
需指出，针对复杂问题，LLM直接给出错误答案的概率比较高。思维链可以看成是一种指令微调。
*
4.2 大模型思维链本质是什么？
. 思维链的本质：利用模型的生成能力和涌现能力，来解决一些复杂或特殊的问题；即将复杂任务拆解为多个简单的子任务，
  它指的是一个思维过程中的连续逻辑推理步骤或关联的序列，是思维过程中一系列相互关联的想法、观点或概念的串联。思维链通常用
于解决问题、做决策或进行推理。它可以按照逻辑顺序连接和组织思维，将复杂的问题分解为更简单的步骤或概念，从而更好地理解和解决问题。
. 适用场景：数学应用题、常识推理、符号操作等
*
4.3 大模型【LLMs】思维链优点是什么？
思维链可以让模型更好地理解问题的含义和范围，更接近人类的思考方式。
*
4.4 大模型【LLMs】思维链类型和策略？
. Few-shot思维链：
. 介绍：即给模型提供一些手动设计的中间步骤或过程，来影响模型的输出；
. 适用场景：这种方法适用于一些相对简单或常见的问题，或者一些模型已经有了很强的涌现能力的问题。
. Zero-shot思维链：
. 介绍：即让模型自动生成中间步骤或过程，然后再根据这些步骤或过程来生成结果。
. 适用场景：这种方法适用于一些相对复杂或特殊的问题，或者一些模型需要更多的引导和调整的问题。
. Least-to-Most思维链：
. 介绍：即给模型提供一个由简单到复杂的提示序列，让模型逐渐增加推理难度和深度；
. 适用场景：这种方法可以让模型更有效地利用其通用知识和涌现能力，同时避免过拟合或灾难性遗忘。
*
4.5 大模型【LLMs】逐步Zero-shot 介绍？
. 逐步 Zero-shot 介绍：指一种利用大型语言模型进行推理的方法，它通过让模型自动生成多个中间步骤或过程，然后再根据
这些步骤或过程来生成结果。
. 逐步 Zero-shot 优点：可以让模型更好地理解问题的含义和范围，更接近人类的思考方式。
在论文LargeLanguageModelsareZero-ShotReasoners中，提出了“逐步 Zero-shot”这一思想。。在第一个阶段，模型首先对问
题进行拆分并分段解答问题（称为ReasoningExtraction）。然后，在第二阶段，模型再进行答案的汇总（称为AnswerExtraction）。
*
4.6 大模型【LLMs】Zero-shot-CoT提示策略定义是什么？
Zero-shot-CoT 提示策略是指一种利用 Zero-shot 思维链进行推理的方法，该方法借助思维链（也被称为思考链，ChainofThought，
CoT）提示法来解决问题。它通过在问题的结尾附加“Let’s think step by step”这几个词，来引导模型生成一个回答问题的思维链。
Zero-shot-CoT 提示策略可以让模型更有效地利用其通用知识和涌现能力，同时避免过拟合或灾难性遗忘。
*
4.7 大模型【LLMs】Zero-shot-CoT提示策略应用方法是什么？
. 首先，明确你想要解决的问题或任务，以及你期望的输出或回答的格式和内容；
. 然后，将你的问题或任务用自然语言描述出来，并在结尾加上“Let’s think step by step”这几个词，作为输入或查询给模型；
. 最后，观察模型生成的输出或回答，看是否符合你的期望和需求。如果不满意，可以尝试修改你的输入或查询，或者使用其他控制参数来影响模型的行为。
例如，你可以使用 Zero-shot-CoT 提示策略来解决一个简单的数学应用题：
-----
提示 我去市场买了 10 个苹果。我给邻居 2 个苹果，给修理工 2 个苹果。然后我又买了 5 个苹果，并吃了 1 个。我还剩下多
少个苹果？Let’s think step by step.
输出 首先，你开始有 10 个苹果。你送走了 2 个苹果给邻居和 2 个苹果给修理工，所以你还剩下 6 个苹果。然后你又买了 5 个
苹果，所以现在你有 11 个苹果。最后，你吃了 1 个苹果，所以你还剩下 10 个苹果。
-----
*
4.8 大模型【LLMs】Few-shot-CoT提示策略定义？
Few-shot-CoT 提示策略是指一种利用 Few-shot 思维链进行推理的方法，它通过给模型提供一些手动设计的中间步骤或过程，
来影响模型的输出。
Few-shot-CoT 提示策略可以让模型更专注于新任务或领域的特点，提高模型的性能和准确度。
*
4.9 大模型【LLMs】Few-shot-CoT提示策略核心思想是什么？
. Few-shot-CoT提示策略核心思想：通过编写思维链样本作为提示词，让模型学会思维链的推导方式，从而更好的完成推导
任务。需要在提示样本中不仅给出问题的答案，还同时需要给出问题推导的过程（即思维链），从而让模型学到思维链的推导过程，
并将其应用到新的问题中。
Few-shot-CoT的方式虽然有效，但是并不是很稳定，如果想要得到稳定的正确答案，可能需要更高阶的提示方法。
*
4.10 大模型【LLMs】Few-shot-CoT提示策略应用方法是什么？
. 首先，明确你想要解决的问题或任务，以及你期望的输出或回答的格式和内容。
. 然后，将你的问题或任务用自然语言描述出来，并在前面加上一些与任务相关的中间步骤或过程，作为输入或查询给模型。
. 最后，观察模型生成的输出或回答，看是否符合你的期望和需求。如果不满意，可以尝试修改你的输入或查询，或者使用其
他控制参数来影响模型的行为。
*
五、大模型【LLMs】涌现现象篇\n5.1 大模型【LLMs】中有一种 涌现现象，你知道么？
**涌现（Emergence）**或称创发、突现、呈展、演生，是一种现象。许多小实体相互作用后产生了大实体，而这个大实体展现了组
成它的小实体所不具有的特性。研究发现，模型规模达到一定阈值以上后，会在多步算术、大学考试、单词释义等场景的准确性显著提
升，称为涌现。
*
5.2 大模型【LLMs】涌现现象主要体现在哪些方面？
. InContextLearning（“Few-ShotPrompt”），即用户给出几个例子，LLM不需要调整模型参数，就能够处理好任务。例
如，用户给出几个情感计算的例子，LLM就能够根据文本判断情感倾向；
. AugmentedPromptingStrategies，即用户使用一些特殊的手段来引导或激发LLM的涌现能力。例如，用户使用多步推理
（chain-of-thoughtprompting）来让LLM进行复杂的逻辑推理；用户使用指令（instructions）来描述任务，而不使用少量示例（few-shot
exemplars）来让LLM进行指令跟随（instructionfollowing）；用户使用程序语言（programminglanguage）来让LLM进行程序执行（program
execution）；
. Zero-ShotorFew-ShotLearning，即LLM能够在没有任何或极少量的训练数据的情况下，解决一些从未见过或者很少见
过的问题。例如，LLM能够根据表情符号解码电影名；LLM能够模拟Linux计算机终端并执行一些简单的数学计算程序。
*
5.3 大模型【LLMs】涌现现象主激活方式？
. 增加模型的规模，即增加模型中参数的数量和复杂度。这可以让模型更好地建立单词之间的联系，更接近人类语言的水平。
一般来说，模型规模越大，涌现能力越强；
. 增加数据的规模，即增加模型训练所用的文本数据的数量和质量。这可以让模型学习到更多的知识和信息，更全面地覆盖各
种领域和场景。一般来说，数据规模越大，涌现能力越广；. 改进模型的架构和训练方法，即使用更先进和有效的神经网络结构和优化算法来构建和训练模型。这可以让模型更灵活和高
效地处理各种任务和问题。一般来说，模型架构和训练方法越优秀，涌现能力越稳定；
. 使用合适的提示（prompt）和反馈（feedback），即根据任务和问题的特点，设计合理和有效的输入输出格式和内容，以及
及时和准确的评估指标和反馈机制。这可以让模型更容易和准确地理解用户的意图和需求，并给出满意的回答。一般来说，提示和反馈
越合适，涌现能力越明显。
*
六、大模型【LLMs】提示工程篇\6.1 大模型【LLMs】提示工程 是什么？
提示工程是指通过设计特殊的提示来激发模型的涌现能力。
这种方法不需要对模型进行额外的训练，只需要通过设计合适的提示来引导模型完成特定任务。提示工程通常用于在不更新模型参
数的情况下，快速解决新问题。
通过输入更加合理的提示，引导模型进行更有效的结果输出，本质上一种引导和激发模型能力的方法
*
6.2 提示工程 如何添加进 大模型【LLMs】？
目前为大模型添加prompt的方式越来越多，主要表现出的一个趋势是，相比于普通的 few-shot 模式（只有输入输出）的 prompt
方式，新的方法会让模型在完成任务的过程中拥有更多的中间过程，例如一些典型的方法：思维链（ChainofThought）、寄存器
（Scratchpad）等等，通过细化模型的推理过程，提高模型的下游任务的效果
*
6.3 微调（FineTuning） 和 提示工程的优缺点？
. 微调（FineTuning） 优缺点：
. 微调的优势:是可以让模型更专注于新任务或领域的特点，提高模型的性能和准确度。微调也可以让模型更容易适应不同的输
入输出格式，以及不同的评估指标。
. 微调的劣势：是需要修改模型的参数，这可能会导致模型过拟合或灾难性遗忘。过拟合是指模型过度适应新数据，而忽略了
预训练模型的通用知识。灾难性遗忘是指模型在学习新数据时，丢失了预训练模型的原有知识。微调也需要一定量的新数据和计算资源，
这可能会增加成本和时间。
. 提示工程优缺点：
. 提示工程的优势：是不需要修改模型的参数，只需要设计合适的输入或查询，就可以引导模型产生期望的输出或回答。提示
工程可以保留模型的通用知识和涌现能力，同时减少数据和计算资源的需求。
. 提示工程的劣势：是需要创造性和细致性，设计有效的提示并不容易。提示工程也可能受到模型本身的限制，无法解决一些
复杂或特殊的问题。提示工程还需要不断地测试和优化，以找到最佳的提示础上，使用少量的新数据来调整模型的参数，以适应新的任
务或领域的过程。
*
6.4 微调（FineTuning） vs 提示工程 在应用场景中关系与联系？
对于这两种方法各自有各自使用的应用场景：
. 提示工程解决的问题，往往不会用微调（如小语义空间内的推理问题）；
. 微调解决的问题，解决那些无法通过特征工程解决的问题。
它们更多的时候是作为上下游技术关系，例如要进行本地知识库的定制化问答，最好的方法就是借助提示工程进行数据标注，然后
再利用标注好的数据进行微调。
*
6.5 代码提示工程的核心概念
代码提示工程的核心概念是指使用特定的文本或代码输入来引导生成式人工智能模型产生期望的代码输出的过程。这种方法不需要
对模型进行额外的训练，只需要通过设计合适的代码提示来引导模型完成特定任务，代码提示工程通常用于解决那些无法通过语言提示
工程解决的问题，也是模型开发中的重中之重。
代码提示工程需要以下几个步骤：. 明确任务的规范，即描述用户想要实现的功能和目标，以及限制和要求。
. 提供相关的上下文，即给出一些与任务相关的背景信息和示例，以帮助模型理解任务的含义和范围。
. 选择合适的模型，即根据任务的难度和复杂度，选择一个适合生成代码的人工智能模型，例如GPT-4或Codex
. 设计有效的提示，即使用合适的词语、短语、符号和格式，来激发模型的生成能力和创造力。
. 测试和优化提示，即在不同的数据和场景下，检验提示的效果和质量，并根据反馈进行调整和改进。
代码提示工程是一个有趣且有用的技能，对于任何使用生成式人工智能模型的人都非常有益。它可以让用户从人工智能模型中获得
最大的收益，通过设计提示来产生理想的代码。
*
6.6 大模型【LLMs】Zero-shot提示方法 是什么？
. Zero-shot提示方法 介绍：给模型提供一个不属于训练数据的提示，但模型可以生成你期望的结果的方法；
. Zero-shot提示方法 优点：使得大型语言模型可以用于许多任务；
eg：可以给模型一个提示，让它翻译一句话，或者给出一个词的定义，或者生成一首诗
. Zero-shot提示方法 使用方式：通过输入一些类似问题和问题答案，让模型参考学习，并在同一个prompt的末尾提出新的问；
. zero-shot可以理解为：不给大模型任何的提示，直接提问，让大模型自己做决策。
*
6.7 大模型【LLMs】Few-shot提示方法 是什么？
. Few-shot提示方法 介绍：给模型提供一些示例或上下文，来引导模型更好地完成任务的方法。这些示例或上下文可以作为
模型的条件，来影响后续的输出；
eg：给模型一些情感分析的示例，让它根据文本判断情感倾向，或者给模型一些编程任务的示例，让它生成代码片段
. 简单理解为：在提问之前，先给大模型一个示例和解释让它学习和模仿，从而在一定程度上赋予它泛化能力。
*
6.8 大模型【LLMs】One-shot和Few-shot提示策略？
. One-shot或者Few-shot提示方法的思想：最简单的提示工程的方法就是通过输入一些类似问题和问题答案，让模型参考学
习，并在同一个prompt的末尾提出新的问题，以此提升模型的推理能力。
*
6.9 大模型【LLMs】One-shot和Few-shot应用场景？
. One-shot提示策略：
. 介绍：只给模型一个示例或上下文的方法；
. 适用场景：适用于一些相对简单或常见的任务，或者一些模型已经有了很强的涌现能力的任务；
. 举例说明：可以给模型一个新词和它的定义，然后让它用这个新词造句。
. Few-shot提示策略：
. 介绍：给模型多个示例或上下文的方法。
. 适用场景：适用于一些相对复杂或特殊的任务，或者一些模型需要更多的引导和调整的任务。
. 举例说明：你可以给模型几个不同类型的笑话，然后让它根据一个关键词生成一个新的笑话。
具体的应用来说，Few-shot提示方法并不复杂，只需要将一些类似的问题的问题+答案作为prompt的一部分进行输入即可。
当需要输入多段问答作为提示词时，以Q作为问题的开头、A作为回答的开头（也可以换成“问题”、“答案”），并且不同的问答对话
需要换行以便于更加清晰的展示，具体方法是通过转义符+换行来完成。
*
微调面\1. 如果想要在某个模型基础上做全参数微调，究竟需要多少显存？
一般 nB的模型，最低需要 16-20nG的显存。（cpuoffload基本不开的情况下）
vicuna-7B为例，官方样例配置为 4*A10040G，测试了一下确实能占满显存。（globalbatchsize128，maxlength2048）当然训练时
用了FSDP、梯度累积、梯度检查点等方式降显存。
*
2. 为什么SFT之后感觉LLM傻了?
. 原版答案：
SFT的重点在于激发大模型的能力，SFT的数据量一般也就是万恶之源alpaca数据集的52k量级，相比于预训练的数据还是太少了。
如果抱着灌注领域知识而不是激发能力的想法，去做SFT的话，可能确实容易把LLM弄傻。
. 新版答案：
指令微调是为了增强（或解锁）大语言模型的能力。
其真正作用：
指令微调后，大语言模型展现出泛化到未见过任务的卓越能力，即使在多语言场景下也能有不错表现 。
*
3.SFT 指令微调数据 如何构建?
. 代表性。应该选择多个有代表性的任务；
. 数据量。每个任务实例数量不应太多（比如：数百个）否则可能会潜在地导致过拟合问题并影响模型性能；
. 不同任务数据量占比。应该平衡不同任务的比例，并且限制整个数据集的容量（通常几千或几万），防止较大的数据集压倒
整个分布。
*
4. 领域模型ContinuePreTrain 数据选取？
技术标准文档或领域相关数据是领域模型ContinuePreTrain的关键。因为领域相关的网站和资讯重要性或者知识密度不如书籍和技术标准。
*
5. 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？
. 动机：仅仅使用领域数据集进行模型训练，模型很容易出现灾难性遗忘现象.
. 解决方法：通常在领域训练的过程中加入通用数据集
那么这个比例多少比较合适呢？
目前还没有一个准确的答案。主要与领域数据量有关系，当数据量没有那么多时，一般领域数据与通用数据的比例在1:5到1:10之间是
比较合适的。
*
6. 领域模型ContinuePreTrain ，如何 让模型在预训练过程中就学习到更多的知识？
领域模型ContinuePreTrain时可以同步加入SFT数据，即MIP，Multi-TaskInstructionPreTraining。
预训练过程中，可以加下游SFT的数据，可以让模型在预训练过程中就学习到更多的知识。
*
7. 进行SFT操作的时候，基座模型选用Chat还是Base?
仅用SFT做领域模型时，资源有限就用在Chat模型基础上训练，资源充足就在Base模型上训练。（资源=数据+显卡）
资源充足时可以更好地拟合自己的数据，如果你只拥有小于10k数据，建议你选用Chat模型作为基座进行微调；如果你拥有100k的数
据，建议你在Base模型上进行微调。
*
8. 领域模型微调 指令&数据输入格式 要求？
在Chat模型上进行SFT时，请一定遵循Chat模型原有的系统指令&数据输入格式。
建议不采用全量参数训练，否则模型原始能力会遗忘较多。
*
9. 领域模型微调时领域评测集如何构建？
领域评测集时必要内容，建议有两份，一份选择题形式自动评测、一份开放形式人工评测。
选择题形式可以自动评测，方便模型进行初筛；开放形式人工评测比较浪费时间，可以用作精筛，并且任务形式更贴近真实场景。
*
10. 领域模型词表扩增是不是有必要的？
领域词表扩增真实解决的问题是解码效率的问题，给模型效果带来的提升可能不会有很大。
*
11. 如何训练自己的大模型？
如果我现在做一个sota的中文GPT大模型，会分2步走：
1. 基于中文文本数据在LLaMA-65B上二次预训练;
2. 加CoT和instruction数据, 用FT+LoRASFT。
提炼下方法，一般分为两个阶段训练：
. 第一阶段：扩充领域词表，比如金融领域词表，在海量领域文档数据上二次预训练LLaMA模型；
. 第二阶段：构造指令微调数据集，在第一阶段的预训练模型基础上做指令精调。还可以把指令微调数据集拼起来成文档格式
放第一阶段里面增量预训练，让模型先理解下游任务信息。
当然，有低成本方案，因为我们有LoRA利器，第一阶段和第二阶段都可以用LoRA训练，如果不用LoRA，就全参微调，大概7B模型
需要8卡A100，用了LoRA后，只需要单卡3090就可以了。
*
12. 训练中文大模型有啥经验？
链家技术报告《TowardsBetterInstructionFollowingLanguageModelsforChinese:InvestigatingtheImpactofTrainingDataand
Evaluation》中，介绍了开源模型的训练和评估方法：还对比了各因素的消融实验：消融实验结论：
. 扩充中文词表后，可以增量模型对中文的理解能力，效果更好
. 数据质量越高越好，而且数据集质量提升可以改善模型效果
. 数据语言分布，加了中文的效果比不加的好
. 数据规模越大且质量越高，效果越好，大量高质量的微调数据集对模型效果提升最明显。解释下：数据量在训练数据量方面，
数据量的增加已被证明可以显著提高性能。值得注意的是，如此巨大的改进可能部分来自belle-3.5和我们的评估数据之间的相似分布。
评估数据的类别、主题和复杂性将对评估结果产生很大影响
. 扩充词表后的LLaMA-7B-EXT的评估表现达到了0.762/0.824=92%的水平
他们的技术报告证明中文大模型的训练是可行的，虽然与ChatGPT还有差距。这里需要指出后续RLHF也很重要，我罗列在这里，抛砖
引玉。
*
13. 指令微调的好处？
有以下好处：
. 对齐人类意图，能够理解自然语言对话（更有人情味）
. 经过微调（fine-tuned），定制版的GPT-3在不同应用中的提升非常明显。OpenAI表示，它可以让不同应用的准确度能直接
从83%提升到95%、错误率可降低50%。解小学数学题目的正确率也能提高2-4倍。（更准）
踩在巨人的肩膀上、直接在1750亿参数的大模型上微调，不少研发人员都可以不用再重头训练自己的AI模型了。（更高效）
*
14. 预训练和微调哪个阶段注入知识的？
预训练阶段注入知识的，微调是在特定任务训练，以使预训练模型的通用知识跟特定任务的要求结合，使模型在特定任务上表现更好。
*
15. 想让模型学习某个领域或行业的知识，是应该预训练还是应该微调？
可以使用预训练和微调相结合的方式，先用篇章数据进行预训练以获取广泛的知识，再用问答对数据进行微调，使模型更好的学习到特
定领域的知识。
当然，GPT大模型的预训练和微调，从实现方式来讲是没有什么差别的，都是decoderonly的语言模型训练并更新参数，如果样本集小，
没有大量的篇章文档数据，我认为只进行微调也能注入知识的，不必太纠结预训练。而且特定领域跟预训练模型的分布差别不大，也不
用二次预训练。
*
16. 多轮对话任务如何微调模型？
跟正常的指令微调方法一样，只是组织数据的时候把历史聊天记录拼接起来，作为上下文信息喂给模型。
{"prompt":"长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号
线", "response": "用电脑能读数据流吗？水温多少", "history": []}
{"prompt":"95","response":"上下水管温差怎么样啊？空气是不是都排干净了呢？","history":[["长城h3风扇不
转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线", "用电脑能读数据流吗？
水温多少"]]}
{"prompt": "是的。上下水管都好的", "response": "那就要检查线路了，一般风扇继电器是由电脑控制吸合的，如果
电路存在断路，或者电脑坏了的话会出现继电器不吸合的情况！","history":[["长城h3风扇不转。继电器好的。保险
丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线", "用电脑能读数据流吗？水温多少"], ["95", "上
下水管温差怎么样啊？空气是不是都排干净了呢？"]]}
*
17. 微调后的模型出现能力劣化，灾难性遗忘是怎么回事？
所谓的灾难性遗忘：即学习了新的知识之后，几乎彻底遗忘掉之前习得的内容。这在微调ChatGLM-6B模型时，有同学提出来的问题，
表现为原始ChatGLM-6B模型在知识问答如“失眠怎么办”的回答上是正确的，但引入特定任务（如拼写纠错CSC）数据集微调后，再让
模型预测“失眠怎么办”的结果就答非所问了。
我理解ChatGLM-6B模型是走完 “预训练-SFT-RLHF” 过程训练后的模型，其SFT阶段已经有上千指令微调任务训练过，现在我们只是
新增了一类指令数据，相对大模型而已，微调数据量少和微调任务类型单一，不会对其原有的能力造成大的影响，所以我认为是不会导
致灾难性遗忘问题，我自己微调模型也没出现此问题。
应该是微调训练参数调整导致的，微调初始学习率不要设置太高，lr=2e-5或者更小，可以避免此问题，不要大于预训练时的学习率。
18. 微调模型需要多大显存？
*
19. 大模型LLM进行SFT操作的时候在学习什么？
（1）预训练->在大量无监督数据上进行预训练，得到基础模型-->将预训练模型作为SFT和RLHF的起点。
(2)SFT-->在有监督的数据集上进行SFT训练，利用上下文信息等监督信号进一步优化模型-->将SFT训练后的模型作为RLHF的起点。
(3)RLHF-->利用人类反馈进行强化学习，优化模型以更好地适应人类意图和偏好-->将RLHF训练后的模型进行评估和验证，并进行必要
的调整。
*
20. 预训练和SFT操作有什么不同
下面使用一个具体的例子进行说明。
问题：描述计算机主板的功能
回答：计算机主板是计算机中的主要电路板。它是系统的支撑。
进行预训练的时候会把这句话连接起来，用前面的词来预测后面出现的词。在计算损失的时候，问句中的损失也会被计算进去。
进行SFT操作则会构建下面这样一条训练语料。
输入：描述计算机主板的功能[BOS]计算机主板是计算机中的主要电路板。它是系统的支撑。[EOS]
标签：[......][BOS]计算机主板是计算机中的主要电路板。它是系统的支撑。[EOS]
其中[BOS]和[EOS]是一些特殊字符，在计算损失时，只计算答句的损失。在多轮对话中，也是一样的，所有的问句损失都会被忽略，而
只计算答句的损失。
因此SFT的逻辑和原来的预训练过程是一致的，但是通过构造一些人工的高质量问答语料，可以高效地教会大模型问答的技巧。
*
21. 样本量规模增大，训练出现OOM错
. 问题描述：模型训练的样本数量从10万，增大300万，训练任务直接报OOM了。
. 解决方案，对数据并行处理，具体实现参考海量数据高效训练，核心思想自定义数据集本次的主要目标是使向量化耗时随着
处理进程的增加线性下降，训练时数据的内存占用只和数据分段大小有关，可以根据数据特点，灵活配置化。核心功能分为以下几点:
. 均分完整数据集到所有进程（总的GPU卡数）
. 每个epoch训练时整体数据分片shuffle一次，在每个进程同一时间只加载单个分段大小数据集
. 重新训练时可以直接加载向量化后的数据。
*
22. 大模型LLM进行SFT 如何对样本进行优化？
. 对于输入历史对话数据进行左截断，保留最新的对话记录。
. 去掉样本中明显的语气词，如嗯嗯，啊啊之类的。
. 去掉样本中不合适的内容，如AI直卖，就不应出现转人工的对话内容。
. 样本中扩充用户特征标签，如年龄，性别，地域，人群等
*
23. 模型参数迭代实验
验证历史对话轮次是否越长越好，通过训练两个模型，控制变量max_source_length｜max_target_length，对训练好之后的模型从Loss、
Bleu指标、离线人工评估等角度进行对比分析。
结论：从人工评估少量样本以及loss下降来看，历史对话长度1024比512长度好，后续如果训练可能上线模型，可以扩大到1024长
度。
*
基于 向量库的文档对话 经验面\一、基于LLM+向量库的文档对话 基础面\1.1 LLMs 存在模型幻觉问题，请问如何处理？
检索+LLM。先用问题在领域数据库里检索到候选答案，再用LLM对答案进行加工。
*
1.2. 基于LLM+向量库的文档对话 思路是怎么样？
. 加载文件
. 读取文本
. 文本分割
. 文本向量化
. 问句向量化
. 在文本向量中匹配出与问句向量最相似的topk个
. 匹配出的文本作为上下文和问题一起添加到 prompt 中
. 提交给 LLM 生成回答
版本一
*
版本二1.3. 基于LLM+向量库的文档对话 核心技术是什么？
. 基于LLM+向量库的文档对话 核心技术：embedding
. 思路：将用户知识库内容经过 embedding 存入向量知识库，然后用户每一次提问也会经过 embedding，利用向量相关性算
法（例如余弦算法）找到最匹配的几个知识库片段，将这些知识库片段作为上下文，与用户问题一起作为 promt 提交给 LLM 回答
*
1.4. 基于LLM+向量库的文档对话 prompt 模板 如何构建？
已知信息：
{context}
根据上述已知信息，简洁和专业的来回答用户的问题。如果无法从中得到答案，
请说 根据已知信息无法回答该问题 或 没有提供足够的相关信息 ，不允许在答案
中添加编造成分，答案请使用中文。
问题是：
{question}
*
二、基于LLM+向量库的文档对话 优化面
痛点1：文档切分粒度不好把控，既担心噪声太多又担心语义信息丢失
问题描述
*
问题1：如何让LLM简要、准确回答细粒度知识？举例及标答如下：用户： 2023年我国上半年的国内生产总值是多少？根据文档， 年的国民生产总值是 亿元。
. 需求分析：一是简要，不要有其他废话。二是准确，而不是随意编造。.
*
问题 2：如何让 LLM 回答出全面的粗粒度（跨段落）知识？
解决方案

需求分析：
要实现语义级别的分割，而不是简单基于html或者pdf的换行符分割。
笔者发现目前的痛点是文档分割不够准确，导致模型有可能只回答了两点，而实际上是因为向量相似度召回的结果是残缺的。
有人可能会问，那完全可以把切割粒度大一点，比如每10个段落一分。但这样显然不是最优的，因为召回片段太大，噪声也就越多。LLM
本来就有幻觉问题，回答得不会很精准（笔者实测也发现如此）。
所以说，我们的文档切片最好是按照语义切割。

思想（原则）
基于LLM的文档对话架构分为两部分，先检索，后推理。重心在检索（推荐系统），推理交给LLM整合即可。
而检索部分要满足三点
①尽可能提高召回率，
②尽可能减少无关信息；
③速度快。
将所有的文本组织成二级索引，第一级索引是 [关键信息]，第二级是 [原始文本]，二者一一映射。
检索部分只对关键信息做embedding，参与相似度计算，把召回结果映射的 原始文本 交给LLM。
. 主要架构图如下：如何构建关键信息？
首先从架构图可以看到，句子、段落、文章都要关键信息，如果为了效率考虑，可以不用对句子构建关键信息。
. 文章的切分及关键信息抽取
. 关键信息: 为各语义段的关键信息集合，或者是各个子标题语义扩充之后的集合（pdf多级标题识别及提取见下一篇文章）
. 语义切分方法1：利用NLP的篇章分析（discourseparsing）工具，提取出段落之间的主要关系，譬如上述极端情况2展示的
段落之间就有从属关系。把所有包含主从关系的段落合并成一段。 这样对文章切分完之后保证每一段在说同一件事情.
. 语义切分方法2：除了discourseparsing的工具外，还可以写一个简单算法利用BERT等模型来实现语义分割。BERT等模型在
预训练的时候采用了NSP（nextsentenceprediction）的训练任务，因此BERT完全可以判断两个句子（段落）是否具有语义衔接关系。这
里我们可以设置相似度阈值t，从前往后依次判断相邻两个段落的相似度分数是否大于t，如果大于则合并，否则断开。当然算法为了效
率，可以采用二分法并行判定，模型也不用很大，笔者用BERT-base-Chinese在中文场景中就取得了不错的效果。
def is_nextsent(sent, next_sent):
    encoding = tokenizer(sent, next_sent,
    return_tensors="pt",truncation=True, padding=False)
    with torch.no_grad():
        outputs = model(**encoding, labels=torch.LongTensor([1]))
        logits = outputs.logits
        probs = torch.softmax(logits/TEMPERATURE, dim=1)
        next_sentence_prob = probs[:, 0].item()if next_sentence_prob <= MERGE_RATIO:
        if next_sentence_prob < MERGE_RATIO:
            return False
        else:
            return True
.语义段的切分及段落（句子）关键信息抽取

如果向量检索效率很高，获取语义段之后完全可以按照真实段落及句号切分，以缓解细粒度知识点检索时大语块噪声多的场景。当然，
关键信息抽取笔者还有其他思路。
. 方法1：利用 NLP 中的成分句法分析（constituencyparsing）工具和命名实体识别（NER）工具提取
. 成分句法分析（constituencyparsing）工具：可以提取核心部分（名词短语、动词短语……）；
. 命名实体识别（NER）工具：可以提取重要实体（货币名、人名、企业名……）。
譬如说：
原始文本： 团队的成员都是精英，核心成员是前谷歌高级产品经理张三，
MM
前 首席技术官李四
meta ……
关键信息：（ 团队，核心成员，张三，李四）
MM
. 方法2：可以用语义角色标注（SemanticRoleLabeling）来分析句子的谓词论元结构，提取“谁对谁做了什么”的信息作为关键
信息。
. 方法3：直接法。其实NLP的研究中本来就有关键词提取工作（KeyphraseExtraction）。也有一个成熟工具可以使用。一个工
具是 HanLP ，中文效果好，但是付费，免费版调用次数有限。还有一个开源工具是KeyBERT，英文效果好，但是中文效果差。
. 方法4：垂直领域建议的方法。以上两个方法在垂直领域都有准确度低的缺陷，垂直领域可以仿照ChatLaw的做法，即：训
练一个生成关键词的模型。ChatLaw就是训练了一个KeyLLM。
常见问题
*
痛点1 句子、语义段、之间召回不会有包含关系吗，是否会造成冗余？
回答：会造成冗余，但是笔者试验之后回答效果很好，无论是细粒度知识还是粗粒度（跨段落）知识准确度都比Longchain粗分效果好很
多，对这个问题笔者认为可以优化但没必要
*
痛点2：在基于垂直领域 表现不佳
模型微调：一个是对embedding模型的基于垂直领域的数据进行微调；一个是对LLM模型的基于垂直领域的数据进行微调；
*
痛点3：如何解决langchain内置问答分句效果不佳的问题？
. 文档加工：
. 一种是使用更好的文档拆分的方式（如项目中已经集成的达摩院的语义识别的模型及进行拆分）；
. 一种是改进填充的方式，判断中心句上下文的句子是否和中心句相关，仅添加相关度高的句子；
. 另一种是文本分段后，对每段分别及进行总结，基于总结内容语义及进行匹配；
*
痛点4：如何尽可能召回与query相关的Document问题
. 问题描述：如何通过得到query相关性高的context，即与query相关的Document尽可能多的能被召回；
. 解决方法：
. 将本地知识切分成Document的时候，需要考虑Document的长度、Documentembedding质量和被召回Document数量这三者
之间的相互影响。在文本切分算法还没那么智能的情况下到的，本地知识的内容最好是已经结构化比较好了，各个段落之间语义关联没那么
强。Document较短的情况下，得Documentembedding的质量可能会高一些，通过Faiss得到的Document与query相关度会高一些。
. 使用Faiss做搜索，前提条件是有高质量的文本向量化工具。因此最好是能基于本地知识对文本向量化工具进行Finetune。另
外也可以考虑将ES搜索结果与Faiss结果相结合。
*
痛点5：如何让LLM基于query和context得到高质量的response
. 问题描述：如何让LLM基于query和context得到高质量的response
. 解决方法：
. 尝试多个的prompt模版，选择一个合适的，但是这个可能有点玄学
. 用与本地知识问答相关的语料，对LLM进行Finetune。
*
langchain面\1. 什么是 LangChain?
LangChain是一个强大的框架，旨在帮助开发人员使用语言模型构建端到端的应用程序。它提供了一套工具、组件和接口，可简化创建由
大型语言模型 (LLM) 和聊天模型提供支持的应用程序的过程。LangChain 可以轻松管理与语言模型的交互，将多个组件链接在一起，并
集成额外的资源，例如 API 和数据库。
*
2. LangChain 包含哪些 核心概念？\2.1 LangChain 中 Components and Chains 是什么？
. Component：模块化的构建块，可以组合起来创建强大的应用程序；
. Chain：组合在一起以完成特定任务的一系列 Components（或其他 Chain）；
注：一个 Chain 可能包括一个 Prompt 模板、一个语言模型和一个输出解析器，
它们一起工作以处理用户输入、生成响应并处理输出。
*
2.2 LangChain 中 Prompt Templates and Values 是什么？.
PromptTemplate 作用：负责创建 PromptValue，这是最终传递给语言模型的内容
PromptTemplate 特点：有助于将用户输入和其他动态信息转换为适合语言模型的格式。PromptValues 是具有方法的类，这
些方法可以转换为每个模型类型期望的确切输入类型（如文本或聊天消息）。
*
2.3 LangChain 中 Example Selectors 是什么？
. 作用：当您想要在 Prompts 中动态包含示例时，ExampleSelectors 很有用。他们接受用户输入并返回一个示例列表以在提示
中使用，使其更强大和特定于上下文。
*
2.4 LangChain 中 Output Parsers 是什么？
. 作用：负责将语言模型响应构建为更有用的格式
. 实现方法：
. 一种用于提供格式化指令
. 另一种用于将语言模型的响应解析为结构化格式
. 特点：使得在您的应用程序中处理输出数据变得更加容易。
*
2.5 LangChain 中 Indexes and Retrievers 是什么？
Index ：一种组织文档的方式，使语言模型更容易与它们交互；
Retrievers：用于获取相关文档并将它们与语言模型组合的接口；
注：LangChain 提供了用于处理不同类型的索引和检索器的工具和功能，例如矢量数据库和文本拆分器。
*
2.6 LangChain 中 Chat Message History 是什么？
. ChatMessageHistory 作用：负责记住所有以前的聊天交互数据，然后可以将这些交互数据传递回模型、汇总或以其他方式组
合；
. 优点：有助于维护上下文并提高模型对对话的理解
*
2.7 LangChain 中 Agents and Toolkits 是什么？
Indexes（索引） 和 Retrievers（检索器） 用于组织和获取文档信息，特别适用于信息检索和问答系统。

索引（Index）：

定义：组织文档的方式，优化数据存储以提升检索效率和准确性。
作用：快速找到与查询相关的文档或信息。
检索器（Retriever）：

定义：根据用户输入从索引中获取相关文档的接口。
作用：确保语言模型生成响应时参考相关背景信息，提高回答的准确性。
实现方法：

索引类型：矢量数据库、文本拆分器等。
检索方法：基于关键词或语义相似性。
*
3. 什么是 LangChain Agent?
. 介绍：LangChainAgent 是框架中驱动决策制定的实体。它可以访问一组工具，并可以根据用户的输入决定调用哪个工具；
. 优点：LangChainAgent 帮助构建复杂的应用程序，这些应用程序需要自适应和特定于上下文的响应。当存在取决于用户输入
和其他因素的未知交互链时，它们特别有用。
*
4. 如何使用 LangChain ?
要使用 LangChain，开发人员首先要导入必要的组件和工具，例如 LLMs,chatmodels,agents,chains, 内存功能。这些组件组合起来创建一
个可以理解、处理和响应用户输入的应用程序。
*
5. LangChain 支持哪些功能?
. 针对特定文档的问答：根据给定的文档回答问题，使用这些文档中的信息来创建答案。
. 聊天机器人：构建可以利用 LLM 的功能生成文本的聊天机器人。
. Agents：开发可以决定行动、采取这些行动、观察结果并继续执行直到完成的代理。
*
6. 什么是 LangChain model?
LangChainmodel 是一种抽象，表示框架中使用的不同类型的模型。LangChain 中的模型主要分为三类：
. LLM（大型语言模型）：这些模型将文本字符串作为输入并返回文本字符串作为输出。它们是许多语言模型应用程序的支柱。
. 聊天模型(ChatModel)：聊天模型由语言模型支持，但具有更结构化的 API。他们将聊天消息列表作为输入并返回聊天消息。
这使得管理对话历史记录和维护上下文变得容易。
. 文本嵌入模型(TextEmbeddingModels)：这些模型将文本作为输入并返回表示文本嵌入的浮点列表。这些嵌入可用于文档检索、
聚类和相似性比较等任务。
开发人员可以为他们的用例选择合适的 LangChain 模型，并利用提供的组件来构建他们的应用程序。
*
7. LangChain 包含哪些特点?
LangChain 旨在为六个主要领域的开发人员提供支持：
. LLM 和提示：LangChain 使管理提示、优化它们以及为所有 LLM 创建通用界面变得容易。此外，它还包括一些用于处理 LLM
的便捷实用程序。
. 链(Chain)：这些是对 LLM 或其他实用程序的调用序列。LangChain 为链提供标准接口，与各种工具集成，为流行应用提供端
到端的链。
. 数据增强生成：LangChain 使链能够与外部数据源交互以收集生成步骤的数据。例如，它可以帮助总结长文本或使用特定数
据源回答问题。
. Agents：Agents 让 LLM 做出有关行动的决定，采取这些行动，检查结果，并继续前进直到工作完成。LangChain 提供了代理
的标准接口，多种代理可供选择，以及端到端的代理示例。
. 内存：LangChain 有一个标准的内存接口，有助于维护链或代理调用之间的状态。它还提供了一系列内存实现和使用内存的
链或代理的示例。
. 评估：很难用传统指标评估生成模型。这就是为什么 LangChain 提供提示和链来帮助开发者自己使用 LLM 评估他们的模型。
*
8.2 LangChain 如何修改 提示模板？
langchain.PromptTemplate:langchain中的提示模板类
根据不同的下游任务设计不同的prompt模板，然后填入内容，生成新的prompt。目的其实就是为了通过设计更准确的提示词，来引导
大模型输出更合理的内容。
*
8.4 LangChain 如何Embedding & vector store？
Emebdding这个过程想必大家很熟悉，简单理解就是把现实中的信息通过各类算法编码成一个高维向量，便于计算机快速计算。
. DL的语言模型建模一般开头都是wordembedding，看情况会加gpositionembeddin。比如咱们的LLM的建模
. 常规检索一般是把refernce数据都先Embedding入库，服务阶段query进来Embedding后再快速在库中查询相似topk。比如
langchain-chatGLM的本地知识库QA系统的入库和检测过程。
. 多模态的方案：同时把语音，文字，图片用不同的模型做Embedding后，再做多模态的模型建模和多模态交互。比如这两天
的Visual-chatGLM.
*
LangChain 存在哪些问题及方法方案？1. LangChain 低效的令牌使用问题
. 问题：Langchain的一个重要问题是它的令牌计数功能，对于小数据集来说，它的效率很低。虽然一些开发人员选择创建自己
的令牌计数函数，但也有其他解决方案可以解决这个问题。
. 解决方案：Tiktoken是OpenAI开发的Python库，用于更有效地解决令牌计数问题。它提供了一种简单的方法来计算文本字符
串中的令牌，而不需要使用像Langchain这样的框架来完成这项特定任务。
*
2. LangChain 文档的问题？
. 问题：文档是任何框架可用性的基石，而Langchain因其不充分且经常不准确的文档而受到指责。误导性的文档可能导致开发
项目中代价高昂的错误，并且还经常有404错误页面。这可能与Langchain还在快速发展有关，作为快速的版本迭代，文档的延后性问题
*
3. LangChain 太多概念容易混淆，过多的“辅助”函数问题？
. 问题：Langchain的代码库因很多概念让人混淆而备受批评，这使得开发人员很难理解和使用它。这种问题的一个方面是存在
大量的“helper”函数，仔细检查就会发现它们本质上是标准Python函数的包装器。开发人员可能更喜欢提供更清晰和直接访问核心功能
的框架，而不需要复杂的中间功能。
简单的分割函数：
*
4. LangChain 行为不一致并且隐藏细节问题
. 问题：LangChain因隐藏重要细节和行为不一致而受到批评，这可能导致生产系统出现意想不到的问题。
eg:LangchainConversationRetrievalChain的一个有趣的方面，它涉及到输入问题的重新措辞。这种重复措辞有时会非常广泛，甚至破坏了
对话的自然流畅性，使对话脱离了上下文。
*
5. LangChain 缺乏标准的可互操作数据类型问题
. 问题：缺乏表示数据的标准方法。这种一致性的缺乏可能会阻碍与其他框架和工具的集成，使其在更广泛的机器学习工具生态系统中工作具有挑战性。
*
LangChain 替代方案？
是否有更好的替代方案可以提供更容易使用、可伸缩性、活动性和特性。
. LlamaIndex是一个数据框架，它可以很容易地将大型语言模型连接到自定义数据源。它可用于存储、查询和索引数据，还提
供了各种数据可视化和分析工具。
. DeepsetHaystack是另外一个开源框架，用于使用大型语言模型构建搜索和问答应用程序。它基于HuggingFaceTransformers，
提供了多种查询和理解文本数据的工具。
*
参数高效微调面(PEFT)1. 微调方法是啥？如何微调？
fine-tune，也叫全参微调，bert微调模型一直用的这种方法，全部参数权重参与更新以适配领域数据，效果好。
prompt-tune, 包括p-tuning、lora、prompt-tuning、adaLoRA等deltatuning方法，部分模型参数参与微调，训练快，显存占用少，效果可能跟FT（fine-tune）比会稍有效果损失，但一般效果能打平。
链家在BELLE的技术报告《AComparativeStudybetweenFull-ParameterandLoRA-basedFine-TuningonChineseInstructionData
forInstructionFollowingLargeLanguageModel》中实验显示：FT效果稍好于LoRA。
peft的论文《ADAPTIVEBUDGETALLOCATIONFORPARAMETER-EFFICIENTFINE-TUNING》显示的结果：AdaLoRA效果稍好于FT。
*
2LoRA 篇2.1LoRA权重是否可以合入原模型？
可以，将训练好的低秩矩阵（B*A）+原模型权重合并（相加），计算出新的权重。
*
2.2ChatGLM-6BLoRA后的权重多大？
rank8target_module query_key_value条件下，大约15M。
*
2.3LoRA 微调 特点
. 基础模型的选择对基于LoRA微调的有效性有显著影响。
. 训练集越多效果越好
. LoRA微调的方法在模型参数越大时体现的优势越明显
注：此结论参考技术报告《AComparativeStudybetweenFull-ParameterandLoRA-basedFine-TuningonChinese
InstructionDataforInstructionFollowingLargeLanguageModel》。
*
2.4LoRA微调方法为啥能加速训练？
. 只更新了部分参数：比如LoRA原论文就选择只更新SelfAttention的参数，实际使用时我们还可以选择只更新部分层的参数；
. 减少了通信时间：由于更新的参数量变少了，所以（尤其是多卡训练时）要传输的数据量也变少了，从而减少了传输时间；
. 采用了各种低精度加速技术，如FP16、FP8或者INT8量化等。
这三部分原因确实能加快训练速度，然而它们并不是LoRA所独有的，事实上几乎都有参数高效方法都具有这些特点。LoRA的优点是
它的低秩分解很直观，在不少场景下跟全量微调的效果一致，以及在预测阶段不增加推理成本。
*
2.5 如何在已有LoRA模型上继续训练？
理解此问题的情形是：已有的lora模型只训练了一部分数据，要训练另一部分数据的话，是在这个lora上继续训练呢，还是跟base 模
型合并后再套一层lora，或者从头开始训练一个lora？我认为把之前的LoRA跟basemodel 合并后，继续训练就可以，为了保留之前的知识和能力，训练新的LoRA时，加入一些之前的训
练数据是需要的。另外，每次都重头来成本高。
*
3. 对比篇3.1 微调方法批处理大小模式GPU显存速度
微调方法批处理大小模式GPU显存速度
LoRA (r=8) 16 FP16 28GB 8ex/s
LoRA (r=8) 8 FP16 24GB 8ex/s
LoRA (r=8) 4 FP16 20GB 8ex/s
LoRA (r=8) 4 INT8 10GB 8ex/s
LoRA (r=8) 4 INT4 8GB 8ex/s
P-Tuning (p=16) 4 FP16 20GB 8ex/s
P-Tuning (p=16) 4 INT8 16GB 8ex/s
P-Tuning (p=16) 4 INT4 12GB 8ex/s
Freeze (l=3) 4 FP16 24GB 8ex/s
Freeze (l=3) 4 INT8 12GB 8ex/s
*
3.2Peft 和 全量微调区别？
所谓的 fune-tine 只能改变风格, 不能改变知识, 是因为我们的 fine-tune, 像是 LoRA 本来就是低秩的, 没办法对模型产生决定性的改
变. 要是全量微调, 还是可以改变知识的.
*
大模型推理面 LLMs1. 为什么大模型推理时显存涨的那么多还一直占着？
. 首先，序列太长了，有很多Q/K/V；
. 其次，因为是逐个预测nexttoken，每次要缓存K/V加速解码。
*
2. 大模型在gpu和cpu上推理速度如何？
7B量级下：
. cpu推理速度约10token/s；
. 单卡A6000和8核AMD的推理速度通常为 10:1。
*
3. 推理速度上，int8和fp16比起来怎么样？
根据实践经验，int8模式一般推理会明显变慢（huggingface的实现）
*
4. 大模型有推理能力吗？
大模型有推理能力。有下面2个方面的体现：
ChatGPT拥有in-contextcorrection的能力，即如果说错了，给出矫正，ChatGPT能“听懂”错在哪儿了，并向正确的方向修正。in-context
correction要比in-contextlearning难了太多，描述越详细清楚，ChatGPT回答得越好。要知道，越详细的描述，在预训练的文本里越难
匹配到的。在询问ChatGPT互联网上并不存在内容的时候，能给出较好答案（如用ChatGPT学建模）；ChatGPT能通过信息猜你心中的想法；你
可以制定一个全新的游戏规则让ChatGPT和你玩，ChatGPT可以理解。
*
5. 大模型生成时的参数怎么设置？
生成模型预测调参建议：
建议去调整下 top_p,num_beams,repetition_renalty,temperature,do_sample=True;
数据生成有重复，调高repetition_renalty；
生成任务表达单一的，样本也不多的，可适当调低 temperature，生成的样子跟训练集的比较像；如果要复现训练集的效果，
temperature=0.01即可。
以上是经验参数，具体调参根据任务而定，不是固定的。
*
6. 有哪些省内存的大语言模型训练/微调/推理方法？
. 动机：大模型（LLMs）现在是 NLP 领域的最主流方法之一，但是大模型的训练/微调/推理需要的内存也越来越多。
举例来说，即使 RTX3090 有着 24GB 的 RAM，是除了 A100 之外显存最大的显卡。但使用一块 RTX3090 依然无法 fp32 精度训
练最小号的 LLaMA-6B。
. Memory-Efficient 的 LLMs 的训练/微调/推理方法
fp16
int8
LoRA.
Gradientcheckpointing
TorchFSDP
CPUoffloading
*
6.1 如何 估算模型所需的RAM？
首先，我们需要了解如何根据参数量估计模型大致所需的 RAM，这在实践中有很重要的参考意义。我们需要通过估算设置 batch_size，
设置模型精度，选择微调方法和参数分布方法等。
接下来，我们用LLaMA-6B 模型为例估算其大致需要的内存。
首先考虑精度对所需内存的影响：
. fp32 精度，一个参数需要 32bits,4bytes.
. fp16 精度，一个参数需要 16bits,2bytes.
. int8 精度，一个参数需要 8bits,1byte.
其次，考虑模型需要的 RAM 大致分三个部分：
. 模型参数
. 梯度
. 优化器参数
. 模型参数：等于参数量*每个参数所需内存。. 对于 fp32，LLaMA-6B 需要 6B*4bytes=24GB内存
. 对于 int8，LLaMA-6B 需要 6B*1byte=6GB
. 梯度：同上，等于参数量*每个梯度参数所需内存。
. 优化器参数：不同的优化器所储存的参数量不同。
对于常用的 AdamW 来说，需要储存两倍的模型参数（用来储存一阶和二阶momentum）。
. fp32 的 LLaMA-6B，AdamW 需要 6B*8bytes=48GB
. int8 的 LLaMA-6B，AdamW 需要 6B*2bytes=12GB
除此之外，CUDAkernel 也会占据一些 RAM，大概 1.3GB 左右，查看方式如下。
> torch.ones((1，1)).to("cuda")
> print_gpu_utilization()
>>>
GPU memory occupied: 1343 MB
综上，int8 精度的 LLaMA-6B 模型部分大致需要 6GB+6GB+12GB+1.3GB=25.3GB 左右。
再根据LLaMA的架构（hidden_size=4096,intermediate_size=11008,num_hidden_layers=32,context_length=2048）计算中间变
量内存。
每个 instance 需要：
(4096 +11008)* 2048 *32 * 1byte = 990MB
所以一张 A100（80GBRAM）大概可以在 int8 精度；batch_size=50 的设定下进行全参数训练。
查看消费级显卡的内存和算力：
2023GPUBenchmarkandGraphicsCardComparisonChart：
https://www.gpucheck.com/gpu-benchmark-graphics-card-comparison-chart
*
6.2Fp16-mixedprecision
混合精度训练的大致思路是在 forwardpass 和 gradientcomputation 的时候使用 fp16 来加速，但是在更新参数时使用 fp32。
用 torch 实现：
CUDAAutomaticMixedPrecisionexamples：https://pytorch.org/docs/stable/notes/amp_examples.html
torchfp16 推理：直接使用 model.half() 将模型转换为fp16.
model.eval()model.half()
使用 HuggingfaceTransformers：在 TrainingArguments 里声明
fp16=Truehttps://huggingface.co/docs/transformers/perf_train_gpu_one#fp16-training
*
6.3Int8-bitsandbytes
Int8 是个很极端的数据类型，它最多只能表示 -128～127 的数字，并且完全没有精度。
为了在训练和 inference 中使用这个数据类型，bitsandbytes 使用了两个方法最大程度地降低了其带来的误差：
.vector-wisequantization
mixedprecisiondecompasition
Huggingface 在这篇文章中用动图解释了 quantization 的实现：https://huggingface.co/blog/hf-bitsandbytes-integration
论文：
LLM.int8():8-bitMatrixMultiplicationforTransformersatScale：https://arxiv.org/abs/2208.07339
借助 HuggingfacePEFT，使用 int8 训练 opt-6.5B 的完整流程：
https://github.com/huggingface/peft/blob/main/examples/int8_training/Finetune_opt_bnb_peft.ipynb
*
6.4 LoRA是什么？
Low-RankAdaptation 是微调 LLMs 最常用的省内存方法之一。LoRA 发现再微调 LLMs 时，更新矩阵（updatematrix）往往特别 sparse，也就是说 updatematrix 是低秩矩阵。LoRA 的作者根据
这一特点将 updatematrixreparametrize 为两个低秩矩阵的积积 。
其中，，A 和 B 的秩为 r，且 。
如此一来，A+B 的参数量将大大小于 .
LoRA 的论文：https://arxiv.org/pdf/2106.09685.pdf
借助 HuggingfacePEFT 框架，使用 LoRA 微调 mt0：
https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq.ipynb
*
6.5GradientCheckpointing
在 torch 中使用 - 把 model 用一个 customize 的 function 包装一下即可，详见：
ExploreGradient-CheckpointinginPyTorch：https://qywu.github.io/2019/05/22/explore-gradient-checkpointing.html
在 HuggingfaceTransformers 中使用：
https://huggingface.co/docs/transformers/v4.27.2/en/perf_train_gpu_one#gradient-checkpointing
*
6.6TorchFSDP+CPUoffload
FullyShardedDataParalle（FSDP）和 DeepSpeed 类似，均通过 ZeRO 等分布优化算法，减少内存的占用量。其将模型参数，梯度
和优化器状态分布至多个 GPU 上，而非像 DDP 一样，在每个 GPU 上保留完整副本。
CPUoffload 则允许在一个 backpropagation 中，将参数动态地从 GPU->CPU,CPU->GPU 进行转移，从而节省 GPU 内存。
Huggingface 这篇博文解释了 ZeRO 的大致实现方法：https://huggingface.co/blog/zero-deepspeed-fairscale
借助 torch 实现 FSDP，只需要将 model 用 FSDPwarp 一下；同样，cpu_offload 也只需要一行代码：
https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/
在这个可以查看 FSDP 支持的模型：https://pytorch.org/docs/stable/fsdp.html
在 HuggingfaceTransformers 中使用 TorchFSDP：
https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/trainer#transformers.Trainin
根据某些 issue，shard_grad_op（只分布保存 optimizerstates 和 gradients）模式可能比 fully_shard 更稳定：
https://github.com/tatsu-lab/stanford_alpaca/issues/32
*
7. 如何让大模型输出合规化
根据用户的输入问题内容，大模型进行生成回答的内容，但是生成的回答，不直接对外输出给用户。需要进行合规的处理，因为大模型
的输出内容不可控，对于严肃的场景，以免引起用户的投诉。所以需要进合并处理。
目前处理的方法，模型生成内容，再把这些内容生成向量，再查询话术向量库，得到最相似的话术。如果查询结果或相似得分比较阈值
低或者查询不到结果，则走兜底策略。兜底策略按用户所在的对话阶段，实验不同的兜底话术。或者使用万能兜底话术。
*
8. 应用模式变更
机器人销售场景的case:
纯大模型AI模式，最初直接是大模型机器人直接和用户对话，全流程都是大模型对话走流程。
对比之前的AI（小模型意图、话术策略）+人工模式，发现之前的初始阶段通过率高些，初步判断可能是用户说的太发散，大模型不好收
敛。
就调整为AI+大模型AI模式。这样前面的AI主要是小模型意图、话术策略模式，任务引导更明确。大模型可以更好的和有意向的用户进
行交互，更容易引导用户成单。
*
分布式训练面\1. 理论篇
1.1 想要训练1个LLM，如果只想用1张显卡，那么对显卡的要求是什么？
显卡显存足够大，nB模型微调一般最好准备20nGB以上的显存。
*
1.2 如果有N张显存足够大的显卡，怎么加速训练？
数据并行（DP），充分利用多张显卡的算力。
*
1.3 如果显卡的显存不够装下一个完整的模型呢？
最直观想法，需要分层加载，把不同的层加载到不同的GPU上（accelerate的device_map）
也就是常见的PP，流水线并行。
*
1.4 PP推理时，是一个串行的过程，1个GPU计算，其他空闲，有没有其他方式？
. 横向切分：流水线并行（PP），也就是分层加载到不同的显卡上。
. 纵向切分：张量并行（TP），在DeepSpeed世界里叫模型并行（MP）
*
1.5 3种并行方式可以叠加吗？
是可以的，DP+TP+PP，这就是3D并行。如果真有1个超大模型需要预训练，3D并行那是必不可少的。毕竟显卡进化的比较慢，最大
显存的也就是A10080g。
单卡80g，可以完整加载小于40B的模型，但是训练时+梯度+优化器状态，5B模型就是上限了，更别说activation的参数也要占显存，
batchsize还得大。而现在100亿以下（10B以下）的LLM只能叫smallLLM。
*
1.6Colossal-AI 有1D/2D/2.5D/3D，是什么情况？
Colossal-AI的nD是针对张量并行，指的是TP的切分，对于矩阵各种切，和3D并行不是一回事。
*
1.7 除了3D并行有没有其他方式大规模训练？
可以使用更优化的数据并行算法FSDP（类似ZeRO3）或者直接使用DeepSpeedZeRO。
*
1.8 有了ZeRO系列，为什么还需要3D并行？
根据ZeRO论文，尽管张量并行的显存更省一点，张量并行的通信量实在太高，只能限于节点内（有NVLINK）。如果节点间张量并行，
显卡的利用率会低到5%
但是，根据Megatron-LM2的论文，当显卡数量增加到千量级，ZeRO3是明显不如3D并行的。
*
1.9 平民适不适合玩3D并行？
不适合。3D并行的基础是，节点内显卡间NVLINK超高速连接才能上TP。有没有NVLINK都是个问题。
而且，节点间特殊的网络通常有400Gb/s？远超普通IDC内的万兆网络10Gb/s。
*
1.10 平民适不适合直接上多机多卡的ZeRO3（万兆网）？
不适合。想象一下，当65B模型用Zero3，每一个step的每一张卡上需要的通信量是195GB（3倍参数量），也就是1560Gb。万兆网下每步也
要156s的通信时间，这画面太美。
*
2. 实践篇
2.1 假如有超多的8卡A100节点（DGXA100），如何应用3D并行策略？
. 首先，张量并行。3种并行方式里，张量并行（TP）对于GPU之间的通信要求最高，而节点内有NVLINK通信速度可以达
到600GB/s。
. 其次，流水线并行，每个节点负责一部分层，每35个节点组成一路完整的流水线，也就是一个完整的模型副本，这里一个模
型副本需280卡。
. 最后，数据并行，官方也做了8路，10路，12路的并行实验，分别使用280个节点，350个节点和420个节点。
参考Megatron-TuringNLG530B
集群规模越大，单个GPU利用率越低。
*
2.2 如果想构这样一个大规模并行训练系统，训练框架如何选？
可以参考Megatron-TuringNLG530B，NVIDIAMegatron-LM+MicrosoftDeepSpeed
BLOOM则是PP+DP用DeepSpeed，TP用Megatron-LM
当然还有一些其他的训练框架，在超大规模下或许也能work。
*
2.3 训练框架如何选？
下面这个图是bloom的一个实验，DP/TP/PP都能降显存，核心是要降到单卡峰值80g以下。
真大模型就是要TP=8，充分利用NVLINK，然后优先PP，最后DP。
然而假大模型（7B）比如LLaMA-7B，可以不用3D并行，直接用DeepSpeedZeRO更方便，参考open-llama项目。
*
3. 并行化策略选择篇
3.1 单GPU
. 显存够用： 直接用
. 显存不够：上offload，用cpu3.2 单节点多卡
. 显存够用（模型能装进单卡）：DDP或ZeRO
. 显存不够：TP或者ZeRO或者PP
重点：没有NVLINK或者NVSwitch，也就是穷人模式，要用PP
3.3 多节点多卡
如果节点间通信速度快（穷人的万兆网肯定不算）
ZeRO或者3D并行，其中3D并行通信量少但是对模型改动大。
如果节点间通信慢，但显存又少。
DP+PP+TP+ZeRO-1
*
4. 问题篇
4.1 推理速度验证
ChatGML在V100单卡的推理耗时大约高出A800单卡推理的40%。
ChatGML推理耗时和问题输出答案的字数关系比较大，答案字数500字以内，A800上大概是每100字，耗时1秒，V100上大概是每
100字，耗时1.4秒。
. ChatGML在A800单卡推理耗时统计. ChatGML在V100单卡推理耗时统计. 结论：
. 训练效率方面: 多机多卡训练，增加训练机器可以线性缩短训练时间。
. 推理性能方面:
. ChatGML在V100单卡的推理耗时大约高出A800单卡推理的40%。
. ChatGML推理耗时和问题输出答案的字数关系比较大，答案字数500字以内，A800上大概是每100字，耗时1秒，V100
上大概是每100字，耗时1.4秒。
*
4.2 并行化训练加速
可采用deepspeed进行训练加速，目前行业开源的大模型很多都是采用的基于deepspeed框架加速来进行模型训练的。如何进行
deepspeed训练，可以参考基于deepspeed构建大模型分布式训练平台。
deepspeed在深度学习模型软件体系架构中所处的位置：‍
DLmodel—>trainopitimization(deepspeed)—>trainframework—>traininstruction(cloud)—>GPUdevice当然需要对比验证deepspeed 的不同参数，选择合适的参数。分别对比stage2,3进行验证，在GPU显存够的情况下，最终使用stage
2。
*
4.3deepspeed 训练过程，报找不主机
解决方法：deepspeed的关联的多机的配置文件，Hostfile 配置中使用ip，不使用hostname
*
4.4 为什么 多机训练效率不如单机？
多机训练可以跑起来，但是在多机上模型训练的速度比单机上还慢。
通过查看服务器相关监控，发现是网络带宽打满，上不去了，其他系统监控基本正常。原理初始的多机之间的网络带宽是64Gps，后面
把多机之间的网络带宽调整为800Gps，问题解决。
实验验证，多机训练的效率，和使用的机器数成线性关系，每台机器的配置一样，如一台GPU机器跑一个epoch需要2小时，4台GPU
机器跑一个epoch需要半小时。除了训练速度符合需求，多机训练模型的loss下降趋势和单机模型训练的趋势基本一致，也符合预期。
*
4.5 多机训练不通，DeepSPeed配置问题
多机间NCCL 不能打通
. 解决方法：
新建 .deepspeed_env 文件，写入如下内容
NCCL_IB_DISABLE=1
NCCL_DEBUG=INFO
NCCL_SOCKET_IFNAME=eth0
NCCL_P2P_DISABLE=1
*
强化学习面
1. 奖励模型需要和基础模型一致吗？
不同实现方式似乎限制不同。（待实践确认）colossal-ai的coati中需要模型有相同的tokenizer，所以选模型只能从同系列中找。在ppo
算法实现方式上据说trlx是最符合论文的
*
显存问题面
1. 大模型大概有多大，模型文件有多大?
一般放出来的模型文件都是fp16的，假设是一个 nB的模型，那么模型文件占 2nG，fp16加载到显存里做推理也是占 2nG，对外的
pr都是 10n 亿参数的模型。
*
2. 能否用4*v10032G训练vicuna65b？
不能。
. 首先，llama65b的权重需要5*v10032G才能完整加载到GPU。
. 其次，vicuna使用flash-attention加速训练，暂不支持v100，需要turing架构之后的显卡。
（刚发现fastchat上可以通过调用train脚本训练vicuna而非train_mem，其实也是可以训练的）
*
3. 如果就是想要试试65b模型，但是显存不多怎么办？
最少大概50g显存，可以在llama-65b-int4（gptq）模型基础上LoRA[6]，当然各种库要安装定制版本的。
*
4.nB模型推理需要多少显存？
考虑模型参数都是fp16，2nG的显存能把模型加载。
*
5.nB模型训练需要多少显存？
基础显存：模型参数+梯度+优化器，总共16nG。
activation占用显存，和maxlen、batchsize有关
解释：优化器部分必须用fp32（似乎fp16会导致训练不稳定），所以应该是2+2+12=16，参考ZeRO论文。
注以上算数不够直观，举个例子？
7B的vicuna在fsdp下总共160G显存勉强可以训练。（按照上面计算7*16=112G是基础显存）
所以全量训练准备显存20nG大概是最低要求，除非内存充足，显存不够offload内存补。
*
6. 如何 估算模型所需的RAM？
首先，我们需要了解如何根据参数量估计模型大致所需的 RAM，这在实践中有很重要的参考意义。我们需要通过估算设置 batch_size，
设置模型精度，选择微调方法和参数分布方法等。
接下来，我们用LLaMA-6B 模型为例估算其大致需要的内存。
首先考虑精度对所需内存的影响：
. fp32 精度，一个参数需要 32bits,4bytes.
. fp16 精度，一个参数需要 16bits,2bytes.
. int8 精度，一个参数需要 8bits,1byte.
其次，考虑模型需要的 RAM 大致分三个部分：
. 模型参数
. 梯度
. 优化器参数
. 模型参数：等于参数量*每个参数所需内存。
. 对于 fp32，LLaMA-6B 需要 6B*4bytes=24GB内存
. 对于 int8，LLaMA-6B 需要 6B*1byte=6GB
. 梯度：同上，等于参数量*每个梯度参数所需内存。
. 优化器参数：不同的优化器所储存的参数量不同。
对于常用的 AdamW 来说，需要储存两倍的模型参数（用来储存一阶和二阶momentum）。
. fp32 的 LLaMA-6B，AdamW 需要 6B*8bytes=48GB
. int8 的 LLaMA-6B，AdamW 需要 6B*2bytes=12GB
除此之外，CUDAkernel 也会占据一些 RAM，大概 1.3GB 左右，查看方式如下。
> torch.ones((1，1)).to("cuda")
> print_gpu_utilization()
>>>
GPU memory occupied: 1343 MB
综上，int8 精度的 LLaMA-6B 模型部分大致需要 6GB+6GB+12GB+1.3GB=25.3GB 左右。
再根据LLaMA的架构（hidden_size=4096,intermediate_size=11008,num_hidden_layers=32,context_length=2048）计算中间变
量内存。每个 instance 需要：
(4096 +11008)* 2048 *32 * 1byte = 990MB
所以一张 A100（80GBRAM）大概可以在 int8 精度；batch_size=50 的设定下进行全参数训练。
查看消费级显卡的内存和算力：
2023GPUBenchmarkandGraphicsCardComparisonChart
https://www.gpucheck.com/gpu-benchmark-graphics-card-comparison-chart
*
7. 如何评估你的显卡利用率
zero3如果没有nvlink，多卡训练下会变慢。但是一直不知道究竟会变得多慢，下面给出几种方法来评估自己在训练时发挥了多少gpu性
能，以及具体测试方法。
*
7.1flops比值法
. 测试工具：deepspeed
. 参考数据：nvidia公布的显卡fp16峰值计算速度（tensorcore）
gpu利用率 = 实测的flops/显卡理论上的峰值flops
举例：deepspeed实测flops100tflops，而用的是A100卡理论峰值312tflops，可以得到GPU利用率只有 32.05%
*
7.2throughout估计法
. 测试工具：手动估算 或者 deepspeed
. 参考数据：论文中的训练速度或者吞吐量
吞吐量 = example数量/秒/GPU * max_length
gpu利用率 = 实际吞吐量 / 论文中的吞吐量（假设利用率100%）
举例：
实测训练时处理样本速度为 3example/s，一共有4卡，maxlength2048，则吞吐量为 1536token/s/gpu
根据llama论文知道，他们训练7B模型的吞吐量约为 3300token/s/gpu，那么GPU利用率只有46.54%
*
7.3torchprofiler分析法
. 测试工具：torchprofiler 及 tensorboard
. 参考数据：无
利用torchprofiler记录各个函数的时间，将结果在tensorboard上展示，在gpukenel视图下，可以看到tensorcore的利用率，比如30%
总结
以上三种方法，在笔者的实验中能得到差不多的利用率指标。
从准确性上看，方案三 > 方案一 > 方案二
从易用性上看，方案二 > 方案一 > 方案三
如果不想改代码就用方案二估算自己的训练速度是不是合理的，如果想精确分析训练速度的瓶颈还是建议使用方案三。
*
8. 测试你的显卡利用率 实现细节篇
8.1 如何查看多机训练时的网速？
iftop命令，看网速很方便。
*
8.2 如何查看服务器上的多卡之间的NVLINKtopo？
$ nvidia-smi topo -m
*
8.3 如何查看服务器上显卡的具体型号?
cd /usr/local/cuda/samples/1_Utilities/deviceQuery
make
./deviceQuery
*
8.4 如何查看训练时的flops？（也就是每秒的计算量）
理论上，如果flops比较低，说明没有发挥出显卡的性能。
如果基于deepspeed训练，可以通过配置文件很方便的测试
{
"flops_profiler": {
"enabled": true,
"profile_step": 1,
"module_depth": -1,
"top_modules": 1,
"detailed": true,
"output_file": null
}
}
参考：https://www.deepspeed.ai/tutorials/flops-profiler/
*
8.5 如何查看对deepspeed的环境配置是否正确？
$ ds_report
*
8.6 tf32格式有多长？
19位
*
1. 大模型大概有多大，模型文件有多大?
一般放出来的模型文件都是fp16的，假设是一个 nB的模型，那么模型文件占 2nG，fp16加载到显存里做推理也是
占 2nG，对外的pr都是 10n 亿参数的模型。
能否用 训练 ？
2.  vicuna 65b能否用 4 * v100 32G训练？
不能。
. 首先，llama65b的权重需要5*v10032G才能完整加载到GPU。
. 其次，vicuna使用flash-attention加速训练，暂不支持v100，需要turing架构之后的显卡。
（刚发现fastchat上可以通过调用train脚本训练vicuna而非train_mem，其实也是可以训练的）
如果就是想要试试 模型，但是显存不多怎么办？
*
3. 如果就是想要试试 模型，但是显存不多怎么办？
65b最少大概50g显存，可以在llama-65b-int4（gptq）模型基础上LoRA[6]，当然各种库要安装定制版本的。
模型推理需要多少显存？
*
4. 模型推理需要多少显存？
考虑模型参数都是fp16，2nG的显存能把模型加载。
模型训练需要多少显存？
*
5. 模型训练需要多少显存？
基础显存：模型参数+梯度+优化器，总共16nG。
activation占用显存，和maxlen、batchsize有关
解释：优化器部分必须用fp32（似乎fp16会导致训练不稳定），所以应该是2+2+12=16，参考ZeRO论文。
注以上算数不够直观，举个例子？
7B的vicuna在fsdp下总共160G显存勉强可以训练。（按照上面计算7*16=112G是基础显存）
所以全量训练准备显存20nG大概是最低要求，除非内存充足，显存不够offload内存补。
如何 估算模型所需的 ？
*
6. RAM
首先，我们需要了解如何根据参数量估计模型大致所需的 RAM，这在实践中有很重要的参考意义。我们需要通过估算
设置 batch_size，设置模型精度，选择微调方法和参数分布方法等。
接下来，我们用LLaMA-6B 模型为例估算其大致需要的内存。
首先考虑精度对所需内存的影响：
. fp32 精度，一个参数需要 32bits,4bytes.
. fp16 精度，一个参数需要 16bits,2bytes.
. int8 精度，一个参数需要 8bits,1byte.
其次，考虑模型需要的 RAM 大致分三个部分：. 模型参数
. 梯度
. 优化器参数
. 模型参数：等于参数量*每个参数所需内存。
. 对于 fp32，LLaMA-6B 需要 6B*4bytes=24GB内存
. 对于 int8，LLaMA-6B 需要 6B*1byte=6GB
. 梯度：同上，等于参数量*每个梯度参数所需内存。
. 优化器参数：不同的优化器所储存的参数量不同。
对于常用的 AdamW 来说，需要储存两倍的模型参数（用来储存一阶和二阶momentum）。
. fp32 的 LLaMA-6B，AdamW 需要 6B*8bytes=48GB
. int8 的 LLaMA-6B，AdamW 需要 6B*2bytes=12GB
除此之外，CUDAkernel 也会占据一些 RAM，大概 1.3GB 左右，查看方式如下。
> torch.ones((1，1)).to("cuda")
> print_gpu_utilization()
>>>
GPU memory occupied: 1343 MB
综上，int8 精度的 LLaMA-6B 模型部分大致需要 6GB+6GB+12GB+1.3GB=25.3GB 左右。
再根据LLaMA的架构（hidden_size=4096,intermediate_size=11008,num_hidden_layers=32,context_length=
2048）计算中间变量内存。
每个 instance 需要：
(4096 +11008)* 2048 *32 * 1byte = 990MB
所以一张 A100（80GBRAM）大概可以在 int8 精度；batch_size=50 的设定下进行全参数训练。
查看消费级显卡的内存和算力：
2023GPUBenchmarkandGraphicsCardComparisonChart
https://www.gpucheck.com/gpu-benchmark-graphics-card-comparison-chart
*
7.如何评估你的显卡利用率？
zero3如果没有nvlink，多卡训练下会变慢。但是一直不知道究竟会变得多慢，下面给出几种方法来评估自己在训练时
发挥了多少gpu性能，以及具体测试方法。
*
7.1 flops比值法？
. 测试工具：deepspeed. 参考数据：nvidia公布的显卡fp16峰值计算速度（tensorcore）
gpu利用率 = 实测的flops/显卡理论上的峰值flops
举例：deepspeed实测flops100tflops，而用的是A100卡理论峰值312tflops，可以得到GPU利用率只有 32.05%
*
7.2 throughout 估计法？
. 测试工具：手动估算 或者 deepspeed
. 参考数据：论文中的训练速度或者吞吐量
吞吐量 = example数量/秒/GPU * max_length
gpu利用率 = 实际吞吐量 / 论文中的吞吐量（假设利用率100%）
举例：
实测训练时处理样本速度为 3example/s，一共有4卡，maxlength2048，则吞吐量为 1536token/s/gpu
根据llama论文知道，他们训练7B模型的吞吐量约为 3300token/s/gpu，那么GPU利用率只有46.54%
*
7.3 torch profiler 分析法
. 测试工具：torchprofiler 及 tensorboard
. 参考数据：无
利用torchprofiler记录各个函数的时间，将结果在tensorboard上展示，在gpukenel视图下，可以看到tensorcore的
利用率，比如30%
总结
以上三种方法，在笔者的实验中能得到差不多的利用率指标。
从准确性上看，方案三 > 方案一 > 方案二
从易用性上看，方案二 > 方案一 > 方案三
如果不想改代码就用方案二估算自己的训练速度是不是合理的，如果想精确分析训练速度的瓶颈还是建议使用方案三。
测试你的显卡利用率 实现细节篇
*
8.8.1 如何查看多机训练时的网速？
iftop命令，看网速很方便。
*
8.2 如何查看服务器上的多卡之间的 NVLINK topo？
$ nvidia-smi topo -m
*
8.3 如何查看服务器上显卡的具体型号?
cd /usr/local/cuda/samples/1_Utilities/deviceQuery
make
./deviceQuery
*
8.4 如何查看训练时的 flops？（也就是每秒的计算量）
理论上，如果flops比较低，说明没有发挥出显卡的性能。
如果基于deepspeed训练，可以通过配置文件很方便的测试
{
"flops_profiler": {
"enabled": true,
"profile_step": 1,
"module_depth": -1,
"top_modules": 1,
"detailed": true,
"output_file": null
}
}
参考：https://www.deepspeed.ai/tutorials/flops-profiler/
*
8.5 如何查看对 deepspeed的环境配置是否正确？
$ds_report
*
8.6 tf32 格式有多长？
19位
8.7 哪里看各类显卡算力比较？
https://lambdalabs.com/gpu-benchmarks
*
8.8 （torch profiler）如何查看自己的训练中通信开销？
用pytorchprofiler查看，下面给出基于transformers的一种快捷的修改方式。
https://github.com/yqhu/profiler-workshop/blob/c8d4a7c30a61cc7b909d89f88f5fd36b70c55769/hf_training_trainer_pr
of.py
用记录的pt.trace.json文件放到tensorboard上，可以看出tensorcore的利用率。
根据实践经验，使用deepspeedzero3时，pcie版本的卡很大部分时间都在通信上，AllGather和ReduceScatter的时
间超过tensorcore计算的时间，所以flops上不去。
*
8.7 哪里看各类显卡算力比较？
https://lambdalabs.com/gpu-benchmarks
*
8.8 （torchprofiler）如何查看自己的训练中通信开销？
用pytorchprofiler查看，下面给出基于transformers的一种快捷的修改方式。https://github.com/yqhu/profiler-workshop/blob/c8d4a7c30a61cc7b909d89f88f5fd36b70c55769/hf_training_trainer_prof.py
用记录的pt.trace.json文件放到tensorboard上，可以看出tensorcore的利用率。
根据实践经验，使用deepspeedzero3时，pcie版本的卡很大部分时间都在通信上，AllGather和ReduceScatter的时间超过tensorcore
计算的时间，所以flops上不去。
*
进阶面
1LLMs 复读机问题
1.1 什么是 LLMs 复读机问题？
LLMs 复读机问题 就是 LLMs 出现 重复输出：
eg：ABCABCABC不断循环输出到max length
*
1.2 为什么会出现 LLMs 复读机问题？
出现 LLMs 复读机问题 的 原因：
prompt部分通常很长，在生成文本时可以近似看作不变，那么条件概率 P(B|A)也不变，一直是最大的。
生成重复内容，是语言模型本身的一个弱点，无论是否微调，都有可能出现。并且，理论上良好的指令微调能够缓解大语言模型生成重
复内容的问题。但是因为指令微调策略的问题，在实践中经常出现指令微调后复读机问题加重的情况。
*
1.3 如何缓解 LLMs 复读机问题？
. 方法一：解码方式里增加不确定性。既然容易复读那我们就增加随机性，开启do_sample选项，调高temperature；
. 方法二：加重复惩罚。如果学的太烂，do_sample也不顶用呢？加重复惩罚，设置repetition_penalty，注意别设置太大了。
不然你会发现连标点符号都不会输出了。
*
2llama 系列问题
2.1llama 输入句子长度理论上可以无限长吗？
限制在训练数据。理论上rope的llama可以处理无限长度，但问题是太长了效果不好啊，没训练过的长度效果通常不好。而想办法让没
训练过的长度效果好，这个问题就叫做“长度外推性”问题。
所以接受2k的长度限制吧。
*
3 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？
Bert 的模型由多层双向的Transformer编码器组成，由12层组成，768隐藏单元，12个head，总参数量110M，约1.15亿参数量。
NLU（自然语言理解）任务效果很好，单卡GPU可以部署，速度快，V100GPU下1秒能处理2千条以上。
ChatGLM-6B,LLaMA-7B模型分别是60亿参数量和70亿参数量的大模型，基本可以处理所有NLP任务，效果好，但大模型部署成本
高，需要大显存的GPU，并且预测速度慢，V100都需要1秒一条。
所以建议：
1）NLU相关的任务，用BERT模型能处理的很好，如实体识别、信息抽取、文本分类，没必要上大模型；
2）NLG任务，纯中文任务，用ChatGLM-6B，需要处理中英文任务，用chinese-alpaca-plus-7b-hf
*
4 各个专业领域是否需要各自的大模型来服务？
是，各行各业的大模型是趋势。
*
5 如何让大模型处理更长的文本？
. 动机：目前绝大多数大模型支持的token最大长度为2048，因为序列长度直接影响Attention的计算复杂度，太长了会影响
训练速度。. 让大模型处理更长的文本 方法
LongChat
就两步：
step1：将新的长度压缩到原来2048长度上，这样的好处是能复用原来的位置信息，增加长度并没有破坏position的权重。
比如从2048扩展到16384，长度变为原来的8倍，那么值为10000的position_id，被压缩成10000/8=1250
代码只需要改一行：
# 将position_ids按比例缩一下。
query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids / ratio).
详细参考：https://kaiokendev.github.io/context
step2：用训练Vicuna的对话语料做微调，超过16k的文本被截断。
. position等比例缩放既然有用，那后续会不会有一种新的position构造的方式，无论多长都可以归一到同样的尺度下，只要
position的相对位置保持不变就可以？其实ALiBi的方法就是一个比较简单优雅的方式，可以部分解决扩展长度的问题。
. 商业模型比如ChatGPT和Claude到底是怎么做的？这个目前都没有公开。首先语料是不缺的，所以只能是结构上的变化。
但是这两个商业模型规模都是100B这个量级的，这么大的参数，如果只增加序列长度而不做其他优化的话，很难训练起来。目前有证
据的方法如下：
. 稀疏化，GPT3的论文中曾提到有这方面的尝试。
. Google的周彦祺在一次分享中透露GPT-4用了MoE的技术(猜测是100B16E)，所以应该有类似的方法来保证在序列变长的
情况下，仍然能高效的训练模型。
. Multi-QueryAttention。Google的PaLM，Falcon等模型都用到过，通过权重共享来提升性能。
. 真正的出路可能还是LinearAttention，将Attention的复杂度从O(N2)降低为O(N). 比如LinearTransformer和RWKV。其
实关于变长序列的问题，历史上现成的解决方案就是RNN，通过信息传递来解决。Transformer的卖点就是AttentionisAllyourNeed，
丢弃了RNN，RWKV敢于把RNN拿回来，还是很有勇气，非常好的一个工作。现在的Attention就有点像历史上的MLP，每个节点之
间都要建立关联，而MLP之后涌现了大量新的结构，所以Transformer是起点，后续肯定会有更合理的结构来取代它。
评测面
*
1 大模型怎么评测？
当前superGLUE,GLUE, 包括中文的CLUE 的benchmark都在不太合适评估大模型。可能评估推理能力、多轮对话能力是核心。
*
2 大模型的honest原则是如何实现的？模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能力？
大模型需要遵循的helpful，honest， harmless的原则。
可以有意构造如下的训练样本，以提升模型准守honest原则，可以算trick了：
微调时构造知识问答类训练集，给出不知道的不回答，加强honest原则；
阅读理解题，读过的要回答，没读过的不回答，不要胡说八道。
*
大模型（ ）训练集面
LLMs
1.SFT（有监督微调）的数据集格式？
一问一答
*
2.RM（奖励模型）的数据格式？
一个问题 + 一条好回答样例 + 一条差回答样例
*
3.PPO（强化学习）的数据格式？理论上来说，不需要新增数据。需要提供一些prompt，可以直接用sft阶段的问。另外，需要限制模型不要偏离原模型太远（ptxloss），
也可以直接用sft的数据。
*
4. 找数据集哪里找？
推荐Alpaca-COT，数据集整理的非常全，眼花缭乱。
*
5. 微调需要多少条数据？
取决于预训练数据和微调任务的数据分布是否一致，分布一致，100条就够，分布差异大就需要多些数据，千条或者万条以上为佳。
自己的任务复杂或者下游任务行业比较冷门，如药品名称识别任务，则需要较多监督数据。还有微调大模型时，一遍是记不住的。100
条的微调数据，epochs=20才能稳定拟合任务要求。
*
6. 有哪些大模型的训练集？
预训练数据集togethercomputer/RedPajama-Data-1T「红睡衣」开源计划总共包括三部分：
. 高质量、大规模、高覆盖度的预训练数据集；
. 在预训练数据集上训练出的基础模型；
. 指令调优数据集和模型，比基本模型更安全、可靠。
预训练数据集RedPajama-Data-1T已开源，包括七个子集，经过预处理后得到的token数量大致可以匹配Meta在原始LLaMA论文中
报告的数量，并且数据预处理相关脚本也已开源。
完整的RedPajama-Data-1T数据集需要的存储容量为压缩后3TB，解压后5TB。
CoT微调数据集：Alpaca-CoT 里面包括常用的alpaca，CoT等数据集，有中文的。
*
7. 进行领域大模型预训练应用哪些数据集比较好？
通过分析发现现有的开源大模型进行预训练的过程中会加入数据、论文等数据。主要是因为这些数据的数据质量较高，领域相关性比较
强，知识覆盖率（密度）较大，可以让模型更适应考试。给我们自己进行大模型预训练的时候提供了一个参考。同时领域相关的网站内
容、新闻内容也是比较重要的数据。
面
*
agent
1. 如何给LLM注入领域知识？
. 方法一：检索+LLM。先用问题在领域数据库里检索到候选答案，再用LLM对答案进行加工。
. 方法二：领域知识微调LLM。把领域知识构建成问答数据集，用SFT让LLM学习这部分知识。
*
2. 如果想要快速体验各种模型，该怎么办？
推荐fastchat，集成了各路开源模型，从自己的vicuna到stableAI的stableLM。
软硬件配置面
*
1 建议的软件环境是什么？
python环境建议3.9，因为fastchat[4]等比较好的开源项目需要3.9及以上。
cuda环境越高越好，c++版本建议直接装9.1.0以上。
*
基础面
1 目前 主流的开源模型体系 有哪些？
目前 主流的开源模型体系 分两种：. 第一种：prefixLM 系
. 代表模型：T5、ChatGLM、ChatGLM2
. 第二种：causalLM 系
. 代表模型：LLaMA-7B、LLaMa 衍生物
*
2prefixLM 和 causalLM 区别是什么？
prefixLM 和 causalLM 区别 在于 attentionmask不同：
. prefixLM：prefix部分的token互相能看到；
. causalLM：严格遵守只有后面的token才能看到前面的token的规则；
*
3 涌现能力是啥原因？
根据前人分析和论文总结，大致是2个猜想：
. 任务的评价指标不够平滑；
. 复杂任务 vs 子任务，这个其实好理解，比如我们假设某个任务 T 有 5 个子任务 Sub-T 构成，每个 sub-T 随着模型增长，
指标从 40% 提升到 60%，但是最终任务的指标只从 1.1% 提升到了 7%，也就是说宏观上看到了涌现现象，但是子任务效果其实是平
滑增长的。
*
4. 大模型LLM的架构介绍？
在自然语言里面有两种模型
. （1）causallanguagemodeling:它的输出是依赖过去和现在的输入。
. （2）maskedlanguagemodeling:它是把句子中的一个词盖住，然后通过这个词的上下文去预测这个词。
大模型的底座模型就是多层的transformer，由于是因果语言模型，它只用了transformer的decoder模块。
*
5. 为何现在的大模型大部分是Decoderonly结构？
因为decoder-only结构模型在没有任何微调数据的情况下，zero-shot的表现能力最好。而encoder-decoder则需要在一定量的标注
数据上做multitask-finetuning才能够激发最佳性能。
目前的LargeLM的训练范式还是在大规模语料shang 做自监督学习，很显然zero-shot性能更好的decoder-only架构才能更好的利用
这些无标注的数据。
大模型使用decoder-only架构除了训练效率和工程实现上的优势外，在理论上因为Encoder的双向注意力会存在低秩的问题，这可能
会削弱模型的表达能力。就生成任务而言，引入双向注意力并无实质的好处。而Encoder-decoder模型架构之所以能够在某些场景下表
现更好，大概是因为它多了一倍参数。所以在同等参数量、同等推理成本下，Decoder-only架构就是最优的选择了。